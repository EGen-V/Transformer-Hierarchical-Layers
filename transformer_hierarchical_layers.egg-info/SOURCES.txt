LICENSE
MANIFEST.in
README.md
pyproject.toml
requirements.txt
setup.py
docs/README.md
docs/THL_ARCHITECTURE_SPEC.md
docs/THL_CONTEXT.md
docs/lang/README_AR.md
docs/lang/README_ES.md
docs/lang/README_FR.md
docs/lang/README_zh-hans.md
docs/research/AUXILIARY-LOSS-FREE LOAD BALANCING STRATEGY FOR MIXTURE-OF-EXPERTS.pdf
docs/research/Advanced Prompt Engineering Techniques for 2025_ Beyond Basic Instructions _ r_PromptEngineering.pdf
docs/research/AirLLM_ Layered Inference for Large Models on Tiny GPUs.pdf
docs/research/DREAMCRAFT3D: HIERARCHICAL 3D GENERATION
WITH BOOTSTRAPPED DIFFUSION PRIOR.pdf
docs/research/DeepSeek LLM
Scaling Open-Source Language Models with Longtermism.pdf
docs/research/DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source
Models in Code Intelligence.pdf
docs/research/DeepSeek-Coder: When the Large Language Model Meets
Programming - The Rise of Code Intelligence.pdf
docs/research/DeepSeek-OCR: Contexts Optical Compression.pdf
docs/research/DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback
for Reinforcement Learning and Monte-Carlo Tree Search.pdf
docs/research/DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via
Reinforcement Learning for Subgoal Decomposition.pdf
docs/research/DeepSeek-Prover: Advancing Theorem Proving in
LLMs through Large-Scale Synthetic Data.pdf
docs/research/DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via
Reinforcement Learning.pdf
docs/research/DeepSeek-V2: A Strong, Economical, and Efficient
Mixture-of-Experts Language Model.pdf
docs/research/DeepSeek-V3 Technical Report.pdf
docs/research/DeepSeek-V3.2: Pushing the Frontier of Open
Large Language Models.pdf
docs/research/DeepSeek-VL2: Mixture-of-Experts Vision-Language Models
for Advanced Multimodal Understanding.pdf
docs/research/DeepSeek-VL: Towards Real-World Vision-Language
Understanding.pdf
docs/research/DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning.pdf
docs/research/DeepSeekMath: Pushing the Limits of Mathematical
Reasoning in Open Language Models.pdf
docs/research/DeepSeekMoE: Towards Ultimate Expert Specialization in
Mixture-of-Experts Language Models.pdf
docs/research/Evaluation of OpenAI o1: Opportunities and Challenges of AGI.pdf
docs/research/Exploring Diffusion Transformer Designs via Grafting.pdf
docs/research/Fire-Flyer AI-HPC: A Cost-Effective
Software-Hardware Co-Design for Deep Learning.pdf
docs/research/Hierarchical Reasoning Model.pdf
docs/research/Hierarchical Reasoning Models in AI and Machine Learning.pdf
docs/research/O: Condensing Reasoning Patterns via Code Input-Output Prediction.pdf
docs/research/Quantifying Memory Utilization with Effective State-Size.pdf
docs/research/Reasoning Models Don’t Always Say What They Think.pdf
docs/research/Research Plan: Reasoning in the THL Algorithm (Focus: Long-Context Memory Reasoning on ARC).md
docs/research/STAR: Synthesis of Tailored Architectures.pdf
docs/research/Transformer Hierarchical Layers (THL): A Non‑Transformer Hierarchical Recurrent Computation Graph with Sparse Routed Memory and Layered Inference for Low‑Budget LLMs.md
docs/research/ieee.LaTeX
examples/README.md
examples/finetune_classification.py
thl/__init__.py
thl/config.py
thl/model.py
thl/tokenizer.py
transformer_hierarchical_layers.egg-info/PKG-INFO
transformer_hierarchical_layers.egg-info/SOURCES.txt
transformer_hierarchical_layers.egg-info/dependency_links.txt
transformer_hierarchical_layers.egg-info/requires.txt
transformer_hierarchical_layers.egg-info/top_level.txt